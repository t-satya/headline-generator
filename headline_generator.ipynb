{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10362479,"sourceType":"datasetVersion","datasetId":6417901},{"sourceId":10364328,"sourceType":"datasetVersion","datasetId":6419251}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:05.842814Z","iopub.execute_input":"2025-01-03T17:12:05.843091Z","iopub.status.idle":"2025-01-03T17:12:06.206367Z","shell.execute_reply.started":"2025-01-03T17:12:05.843060Z","shell.execute_reply":"2025-01-03T17:12:06.205518Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/d/iitm21f1000641/logicloom/LABELLED_DEV.csv\n/kaggle/input/d/iitm21f1000641/logicloom/LABELLED_TRAIN.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers accelerate evaluate rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:06.207094Z","iopub.execute_input":"2025-01-03T17:12:06.207379Z","iopub.status.idle":"2025-01-03T17:12:09.550171Z","shell.execute_reply.started":"2025-01-03T17:12:06.207358Z","shell.execute_reply":"2025-01-03T17:12:09.549302Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\ntrain = load_dataset(\"csv\",data_files=\"/kaggle/input/d/iitm21f1000641/logicloom/LABELLED_TRAIN.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:09.551336Z","iopub.execute_input":"2025-01-03T17:12:09.551614Z","iopub.status.idle":"2025-01-03T17:12:10.385515Z","shell.execute_reply.started":"2025-01-03T17:12:09.551589Z","shell.execute_reply":"2025-01-03T17:12:10.384749Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(type(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.386149Z","iopub.execute_input":"2025-01-03T17:12:10.386511Z","iopub.status.idle":"2025-01-03T17:12:10.397313Z","shell.execute_reply.started":"2025-01-03T17:12:10.386486Z","shell.execute_reply":"2025-01-03T17:12:10.395094Z"}},"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"eval = load_dataset(\"csv\",data_files=\"/kaggle/input/d/iitm21f1000641/logicloom/LABELLED_DEV.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.402613Z","iopub.execute_input":"2025-01-03T17:12:10.405155Z","iopub.status.idle":"2025-01-03T17:12:10.451975Z","shell.execute_reply.started":"2025-01-03T17:12:10.405126Z","shell.execute_reply":"2025-01-03T17:12:10.451398Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dataset = train.filter(lambda example: example[\"News Article\"] is not None and example[\"Caption\"] is not None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.453066Z","iopub.execute_input":"2025-01-03T17:12:10.453245Z","iopub.status.idle":"2025-01-03T17:12:10.459376Z","shell.execute_reply.started":"2025-01-03T17:12:10.453229Z","shell.execute_reply":"2025-01-03T17:12:10.458607Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"eval_dataset = eval.filter(lambda example: example[\"News Article\"] is not None and example[\"Caption\"] is not None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.460049Z","iopub.execute_input":"2025-01-03T17:12:10.460237Z","iopub.status.idle":"2025-01-03T17:12:10.472852Z","shell.execute_reply.started":"2025-01-03T17:12:10.460221Z","shell.execute_reply":"2025-01-03T17:12:10.472084Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)  # Remove special characters\n    return text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.473688Z","iopub.execute_input":"2025-01-03T17:12:10.473915Z","iopub.status.idle":"2025-01-03T17:12:10.478978Z","shell.execute_reply.started":"2025-01-03T17:12:10.473897Z","shell.execute_reply":"2025-01-03T17:12:10.478243Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def normalize_whitespace(text):\n    return \" \".join(text.split())\ndef to_lowercase(text):\n    return text.lower()\ndef preprocess_text(text):\n    text = clean_text(text)\n    text = normalize_whitespace(text)\n    text = to_lowercase(text)  # Optional, based on use case\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.479659Z","iopub.execute_input":"2025-01-03T17:12:10.479898Z","iopub.status.idle":"2025-01-03T17:12:10.489588Z","shell.execute_reply.started":"2025-01-03T17:12:10.479878Z","shell.execute_reply":"2025-01-03T17:12:10.488909Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n\ndef preprocess_function(examples):\n    inputs = [preprocess_text(text) for text in examples[\"News Article\"]]\n    targets = [preprocess_text(text) for text in examples[\"Caption\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n    labels = tokenizer(targets, max_length=128, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.490336Z","iopub.execute_input":"2025-01-03T17:12:10.490791Z","iopub.status.idle":"2025-01-03T17:12:10.502383Z","shell.execute_reply.started":"2025-01-03T17:12:10.490762Z","shell.execute_reply":"2025-01-03T17:12:10.501543Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\ndef preprocess_function(examples):\n    inputs = examples[\"News Article\"]\n    targets = examples[\"Caption\"]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True,padding=\"max_length\",\n                             return_tensors=\"pt\")\n    labels = tokenizer(targets, max_length=128, truncation=True,padding=\"max_length\",\n                      return_tensors=\"pt\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset_train = train_dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:10.503161Z","iopub.execute_input":"2025-01-03T17:12:10.503368Z","iopub.status.idle":"2025-01-03T17:12:14.780340Z","shell.execute_reply.started":"2025-01-03T17:12:10.503350Z","shell.execute_reply":"2025-01-03T17:12:14.779652Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\ntokenized_dataset_eval = eval_dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:14.781109Z","iopub.execute_input":"2025-01-03T17:12:14.781430Z","iopub.status.idle":"2025-01-03T17:12:15.467956Z","shell.execute_reply.started":"2025-01-03T17:12:14.781408Z","shell.execute_reply":"2025-01-03T17:12:15.467085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba697aeba0b42b5b1cf6e9d12440486"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport torch\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:15.468842Z","iopub.execute_input":"2025-01-03T17:12:15.469081Z","iopub.status.idle":"2025-01-03T17:12:18.793294Z","shell.execute_reply.started":"2025-01-03T17:12:15.469059Z","shell.execute_reply":"2025-01-03T17:12:18.792499Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    logging_dir='./logs',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:18.794147Z","iopub.execute_input":"2025-01-03T17:12:18.794878Z","iopub.status.idle":"2025-01-03T17:12:21.510310Z","shell.execute_reply.started":"2025-01-03T17:12:18.794829Z","shell.execute_reply":"2025-01-03T17:12:21.509655Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset_train,\n    eval_dataset=tokenized_dataset_eval,\n    tokenizer=tokenizer,\n)\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:12:21.511216Z","iopub.execute_input":"2025-01-03T17:12:21.511442Z","iopub.status.idle":"2025-01-03T17:12:22.819235Z","shell.execute_reply.started":"2025-01-03T17:12:21.511423Z","shell.execute_reply":"2025-01-03T17:12:22.818546Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom torch.optim import Adam\n\n# Initialize the optimizer\noptimizer = Adam(model.parameters(), lr=5e-5)  # Adjust learning rate as needed\n\n# Use the dataset already tokenized and passed to the trainer\n# train_dataset = tokenized_dataset_train\n\n# Initialize the data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,\n)\n\n# Create DataLoader\ntrain_dataloader = torch.utils.data.DataLoader(\n    tokenized_dataset_train[\"train\"],\n    batch_size=8,  # Adjust batch size as needed\n    collate_fn=lambda batch: {\n        key: torch.tensor([item[key] for item in batch], dtype=torch.long) \n        if isinstance(batch[0].get(key), list) else batch[0][key] \n        for key in batch[0]\n    }\n)\nval_dataloader = torch.utils.data.DataLoader(\n    tokenized_dataset_eval[\"train\"],\n    batch_size=8,  # Adjust batch size as needed\n    collate_fn=lambda batch: {\n        key: torch.tensor([item[key] for item in batch], dtype=torch.long) \n        if isinstance(batch[0].get(key), list) else batch[0][key] \n        for key in batch[0]\n    }\n)\n\nmodel = trainer.model\nmodel.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:13:50.345161Z","iopub.execute_input":"2025-01-03T17:13:50.345448Z","iopub.status.idle":"2025-01-03T17:13:50.362701Z","shell.execute_reply.started":"2025-01-03T17:13:50.345425Z","shell.execute_reply":"2025-01-03T17:13:50.361256Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"MT5ForConditionalGeneration(\n  (shared): Embedding(250112, 512)\n  (encoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"print(type(tokenized_dataset_train))\ntokenized_dataset_train\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:13:57.169008Z","iopub.execute_input":"2025-01-03T17:13:57.169313Z","iopub.status.idle":"2025-01-03T17:13:57.175025Z","shell.execute_reply.started":"2025-01-03T17:13:57.169284Z","shell.execute_reply":"2025-01-03T17:13:57.174224Z"}},"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'News Article', 'Caption', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 3000\n    })\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print(type(tokenized_dataset_eval))\ntokenized_dataset_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:13:27.369724Z","iopub.execute_input":"2025-01-03T17:13:27.370043Z","iopub.status.idle":"2025-01-03T17:13:27.376649Z","shell.execute_reply.started":"2025-01-03T17:13:27.370017Z","shell.execute_reply":"2025-01-03T17:13:27.375955Z"}},"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'News Article', 'Caption', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Initialize the best validation loss to a large number\nbest_val_loss = float('inf')\n\n# Custom training loop\nfor epoch in range(int(training_args.num_train_epochs)):\n    print(f\"Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\")\n    epoch_loss = 0  # To track the epoch loss\n    \n    for batch in tqdm(train_dataloader):\n        # Move data to the appropriate device (GPU/CPU)\n        batch = {k: v.to(training_args.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n      \n        # Remove any keys that are not needed by the model\n        batch = {k: v for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n   \n        # Forward pass\n        outputs = model(**batch)\n        loss = outputs.loss\n        epoch_loss += loss.item()\n\n        # Backward pass\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Print training loss after each epoch\n    print(f\"Training loss after epoch {epoch + 1}: {epoch_loss / len(train_dataloader)}\")\n    \n    # Now calculate validation loss after each epoch\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0\n    with torch.no_grad():  # Disable gradients for validation to save memory\n        for batch in val_dataloader:\n            batch = {k: v.to(training_args.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}  \n\n            batch = {k: v for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n\n            outputs = model(**batch)\n            loss = outputs.loss\n            val_loss += loss.item()\n\n    # Print validation loss after each epoch\n    print(f\"Validation loss after epoch {epoch + 1}: {val_loss / len(val_dataloader)}\")\n    \n    # Check if the current validation loss is better (lower) than the best so far\n    if val_loss / len(val_dataloader) < best_val_loss:\n        best_val_loss = val_loss / len(val_dataloader)\n        print(f\"Validation loss decreased to {best_val_loss}. Saving model...\")\n\n        # Save the model's state_dict and tokenizer\n        torch.save(model.state_dict(), \"best_model.pth\")  # Save the model state_dict\n        tokenizer.save_pretrained(\"best_model\")  # Save the tokenizer\n\n        # Optionally, you can also save the optimizer if you want to resume training later\n        torch.save(optimizer.state_dict(), \"best_model_optimizer.pt\")\n        print(\"Best model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:29:15.158223Z","iopub.execute_input":"2025-01-03T17:29:15.158561Z","iopub.status.idle":"2025-01-03T17:43:07.163092Z","shell.execute_reply.started":"2025-01-03T17:29:15.158530Z","shell.execute_reply":"2025-01-03T17:43:07.162149Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training loss after epoch 1: 0.9971842927932739\nValidation loss after epoch 1: 0.9833337903022766\nValidation loss decreased to 0.9833337903022766. Saving model...\nBest model saved.\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training loss after epoch 2: 0.8326277588208516\nValidation loss after epoch 2: 0.9113809800148011\nValidation loss decreased to 0.9113809800148011. Saving model...\nBest model saved.\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [04:06<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training loss after epoch 3: 0.8502487783432007\nValidation loss after epoch 3: 0.9155899205207825\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(train_dataset[0])  # Replace with your dataset object\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T17:02:46.162130Z","iopub.status.idle":"2025-01-03T17:02:46.162452Z","shell.execute_reply":"2025-01-03T17:02:46.162300Z"}},"outputs":[],"execution_count":null}]}